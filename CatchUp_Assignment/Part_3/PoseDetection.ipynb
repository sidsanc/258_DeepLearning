{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVCn44NLX/YW1jYAn0I/6R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidsanc/258_DeepLearning/blob/main/CatchUp_Assignment/Part_3/PoseDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10_zkgbZBkIE"
      },
      "source": [
        "# Human Pose Estimation with MoveNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u_VGR6_BmbZ"
      },
      "source": [
        "## Visualization libraries & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtcwSIcgbIVN"
      },
      "source": [
        "!pip install -q imageio\n",
        "!pip install -q opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BLeJv-pCCld"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow_docs.vis import embed\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Import matplotlib libraries\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.collections import LineCollection\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython.display import HTML, display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvrN0iQiOxhR"
      },
      "source": [
        "## Load Model from TF hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeGHgANcT7a1"
      },
      "source": [
        "model_name = \"movenet_lightning\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
        "\n",
        "if \"tflite\" in model_name:\n",
        "  if \"movenet_lightning_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_f16\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  elif \"movenet_lightning_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder_int8\" in model_name:\n",
        "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    # TF Lite format expects tensor type of uint8.\n",
        "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
        "    # Invoke inference.\n",
        "    interpreter.invoke()\n",
        "    # Get the model prediction.\n",
        "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
        "    return keypoints_with_scores\n",
        "\n",
        "else:\n",
        "  if \"movenet_lightning\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
        "    input_size = 192\n",
        "  elif \"movenet_thunder\" in model_name:\n",
        "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
        "    input_size = 256\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
        "\n",
        "  def movenet(input_image):\n",
        "    \"\"\"Runs detection on an input image.\n",
        "\n",
        "    Args:\n",
        "      input_image: A [1, height, width, 3] tensor represents the input image\n",
        "        pixels. Note that the height/width should already be resized and match the\n",
        "        expected input resolution of the model before passing into this function.\n",
        "\n",
        "    Returns:\n",
        "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
        "      coordinates and scores.\n",
        "    \"\"\"\n",
        "    model = module.signatures['serving_default']\n",
        "\n",
        "    # SavedModel format expects tensor type of int32.\n",
        "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
        "    # Run model inference.\n",
        "    outputs = model(input_image)\n",
        "    # Output is a [1, 1, 17, 3] tensor.\n",
        "    keypoints_with_scores = outputs['output_0'].numpy()\n",
        "    return keypoints_with_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h1qHYaqD9ap"
      },
      "source": [
        "## Single Image Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I3xBq80E3N_"
      },
      "source": [
        "### Load Input Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMO4B-wx5psP"
      },
      "source": [
        "!curl -o input_image.jpeg https://images.pexels.com/photos/4384679/pexels-photo-4384679.jpeg --silent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJZYQ8KYFQ6x"
      },
      "source": [
        "# Load the input image.\n",
        "image_path = 'input_image.jpeg'\n",
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_jpeg(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_UWRdQxE6WN"
      },
      "source": [
        "### Run Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHmTwACwFW-v"
      },
      "source": [
        "# Resize and pad the image to keep the aspect ratio and fit the expected size.\n",
        "input_image = tf.expand_dims(image, axis=0)\n",
        "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
        "\n",
        "# Run model inference.\n",
        "keypoints_with_scores = movenet(input_image)\n",
        "\n",
        "# Visualize the predictions with image.\n",
        "display_image = tf.expand_dims(image, axis=0)\n",
        "display_image = tf.cast(tf.image.resize_with_pad(\n",
        "    display_image, 1280, 1280), dtype=tf.int32)\n",
        "output_overlay = draw_prediction_on_image(\n",
        "    np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(output_overlay)\n",
        "_ = plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKm-B0eMYeg8"
      },
      "source": [
        "## Video (Image Sequence) Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2JmA1xAEntQ"
      },
      "source": [
        "### Load Input Image Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzJxbxDckWl2"
      },
      "source": [
        "!wget -q -O dance.gif https://github.com/tensorflow/tfjs-models/raw/master/pose-detection/assets/dance_input.gif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxbMFZJUkd6W"
      },
      "source": [
        "# Load the input image.\n",
        "image_path = 'dance.gif'\n",
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_gif(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJKeQ4siEtU9"
      },
      "source": [
        "### Run Inference with Cropping Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B57XS0NZPIy"
      },
      "source": [
        "# Load the input image.\n",
        "num_frames, image_height, image_width, _ = image.shape\n",
        "crop_region = init_crop_region(image_height, image_width)\n",
        "\n",
        "output_images = []\n",
        "bar = display(progress(0, num_frames-1), display_id=True)\n",
        "for frame_idx in range(num_frames):\n",
        "  keypoints_with_scores = run_inference(\n",
        "      movenet, image[frame_idx, :, :, :], crop_region,\n",
        "      crop_size=[input_size, input_size])\n",
        "  output_images.append(draw_prediction_on_image(\n",
        "      image[frame_idx, :, :, :].numpy().astype(np.int32),\n",
        "      keypoints_with_scores, crop_region=None,\n",
        "      close_figure=True, output_image_height=300))\n",
        "  crop_region = determine_crop_region(\n",
        "      keypoints_with_scores, image_height, image_width)\n",
        "  bar.update(progress(frame_idx, num_frames-1))\n",
        "\n",
        "# Prepare gif visualization.\n",
        "output = np.stack(output_images, axis=0)\n",
        "to_gif(output, duration=100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}