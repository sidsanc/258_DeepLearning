{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0c/OxhXW8kjBYkTlsazcX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidsanc/258_DeepLearning/blob/main/CatchUp_Assignment/Part_3/DreamBooth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z9WGMwpo22T"
      },
      "source": [
        "# DreamBooth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZFd7s-2o22a"
      },
      "outputs": [],
      "source": [
        "from accelerate.utils import write_basic_config\n",
        "\n",
        "write_basic_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeomiWalo22c"
      },
      "source": [
        "Finally, download a [few images of a dog](https://huggingface.co/datasets/diffusers/dog-example) to DreamBooth with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW1r5MP0o22k"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "local_dir = \"./dog\"\n",
        "snapshot_download(\n",
        "    \"diffusers/dog-example\",\n",
        "    local_dir=local_dir,\n",
        "    repo_type=\"dataset\",\n",
        "    ignore_patterns=\".gitattributes\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSM5Yd4Qo22l"
      },
      "source": [
        "To use your own dataset, take a look at the [Create a dataset for training](https://huggingface.co/docs/diffusers/main/en/training/create_dataset) guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrnxUrtno22l"
      },
      "source": [
        "## Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvsRj1mxo22m"
      },
      "source": [
        "## Finetuning with prior-preserving loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYBlxIR3o22n"
      },
      "source": [
        "## Finetuning the text encoder and UNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVEU6kfso22o"
      },
      "source": [
        "## Finetuning with LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRbHA4SWo22o"
      },
      "source": [
        "## Saving checkpoints while training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCzPQValo22q"
      },
      "source": [
        "### Inference from a saved checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3ujc-zTo22r"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline, UNet2DConditionModel\n",
        "from transformers import CLIPTextModel\n",
        "import torch\n",
        "\n",
        "# Load the pipeline with the same arguments (model, revision) that were used for training\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "unet = UNet2DConditionModel.from_pretrained(\"/sddata/dreambooth/daruma-v2-1/checkpoint-100/unet\")\n",
        "\n",
        "# if you have trained with `--args.train_text_encoder` make sure to also load the text encoder\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"/sddata/dreambooth/daruma-v2-1/checkpoint-100/text_encoder\")\n",
        "\n",
        "pipeline = DiffusionPipeline.from_pretrained(model_id, unet=unet, text_encoder=text_encoder, dtype=torch.float16)\n",
        "pipeline.to(\"cuda\")\n",
        "\n",
        "# Perform inference, or save, or push to the hub\n",
        "pipeline.save_pretrained(\"dreambooth-pipeline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya9urVeio22r"
      },
      "source": [
        "If you have **`\"accelerate<0.16.0\"`** installed, you need to convert it to an inference pipeline first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Y8RGkDo22r"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "# Load the pipeline with the same arguments (model, revision) that were used for training\n",
        "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
        "pipeline = DiffusionPipeline.from_pretrained(model_id)\n",
        "\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Use text_encoder if `--train_text_encoder` was used for the initial training\n",
        "unet, text_encoder = accelerator.prepare(pipeline.unet, pipeline.text_encoder)\n",
        "\n",
        "# Restore state from a checkpoint path. You have to use the absolute path here.\n",
        "accelerator.load_state(\"/sddata/dreambooth/daruma-v2-1/checkpoint-100\")\n",
        "\n",
        "# Rebuild the pipeline with the unwrapped models (assignment to .unet and .text_encoder should work too)\n",
        "pipeline = DiffusionPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    unet=accelerator.unwrap_model(unet),\n",
        "    text_encoder=accelerator.unwrap_model(text_encoder),\n",
        ")\n",
        "\n",
        "# Perform inference, or save, or push to the hub\n",
        "pipeline.save_pretrained(\"dreambooth-pipeline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZghwbdGo22v"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fakYQZvCo22w"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"path_to_saved_model\"\n",
        "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")\n",
        "\n",
        "prompt = \"A photo of sks dog in a bucket\"\n",
        "image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
        "\n",
        "image.save(\"dog-bucket.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etfHZtHLo22w"
      },
      "source": [
        "You may also run inference from any of the [saved training checkpoints](#inference-from-a-saved-checkpoint)."
      ]
    }
  ]
}